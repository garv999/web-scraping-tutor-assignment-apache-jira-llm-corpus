{"type":"summary","metadata":{"issue_key":"SPARK-515","project":"Unknown","issue_type":"Bug","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: SPARK-515\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Make it easier to run external code on a Nexus cluster\n\nDescription:\nRight now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).\n","output":"Issue SPARK-515 is a bug in the Unknown project. Make it easier to run external code on a Nexus cluster This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"SPARK-515","project":"Unknown","actual_type":"Bug","actual_priority":""},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Make it easier to run external code on a Nexus cluster\n\nDescription: Right now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).","output":{"issue_type":"Bug","priority":"","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"SPARK-515","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: SPARK-515\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Make it easier to run external code on a Nexus cluster\n\nDescription:\nRight now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).\n","qa_pairs":[{"question":"What is the main problem described in issue SPARK-515?","answer":"Make it easier to run external code on a Nexus cluster"},{"question":"What is the current status of issue SPARK-515?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue SPARK-515?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"SPARK-515","project":"Unknown","comment_count":3},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Make it easier to run external code on a Nexus cluster","comments":"Comment 1 by Patrick McFadin:\nGithub comment from tjhunter: I have a (partial) implementation of this feature. Please consider reviewing for future merging.\n\nhttps://github.com/tjhunter/spark/commit/8587497497dc825e0d0a312ff156f1853348aebf\n\nComment 2 by Patrick McFadin:\nGithub comment from mateiz: Closing this now that it is possible to pass jars and sparkHome to SparkContext's constructor.\n\nComment 3 by Patrick McFadin:\nImported from Github issue spark-1, originally reported by mateiz"},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":3}}
{"type":"key_extraction","metadata":{"issue_key":"SPARK-515","project":"Unknown"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Make it easier to run external code on a Nexus cluster\n\nRight now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"SPARK-514","project":"Unknown","issue_type":"Bug","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: SPARK-514\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Merge fault tolerance code into master branch\n","output":"Issue SPARK-514 is a bug in the Unknown project. Merge fault tolerance code into master branch This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"SPARK-514","project":"Unknown","actual_type":"Bug","actual_priority":""},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Merge fault tolerance code into master branch\n\nDescription: ","output":{"issue_type":"Bug","priority":"","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"SPARK-514","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: SPARK-514\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Merge fault tolerance code into master branch\n","qa_pairs":[{"question":"What is the main problem described in issue SPARK-514?","answer":"Merge fault tolerance code into master branch"},{"question":"What is the current status of issue SPARK-514?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue SPARK-514?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"SPARK-514","project":"Unknown","comment_count":2},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Merge fault tolerance code into master branch","comments":"Comment 1 by Patrick McFadin:\nGithub comment from mateiz: Done.\n\nComment 2 by Patrick McFadin:\nImported from Github issue spark-2, originally reported by mateiz"},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":2}}
{"type":"summary","metadata":{"issue_key":"SPARK-513","project":"Unknown","issue_type":"Bug","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: SPARK-513\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Make NexusScheduler more efficient by keeping a list of tasks for each node\n\nDescription:\nRight now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.\n","output":"Issue SPARK-513 is a bug in the Unknown project. Make NexusScheduler more efficient by keeping a list of tasks for each node This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"SPARK-513","project":"Unknown","actual_type":"Bug","actual_priority":""},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Make NexusScheduler more efficient by keeping a list of tasks for each node\n\nDescription: Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.","output":{"issue_type":"Bug","priority":"","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"SPARK-513","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: SPARK-513\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Make NexusScheduler more efficient by keeping a list of tasks for each node\n\nDescription:\nRight now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.\n","qa_pairs":[{"question":"What is the main problem described in issue SPARK-513?","answer":"Make NexusScheduler more efficient by keeping a list of tasks for each node"},{"question":"What is the current status of issue SPARK-513?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue SPARK-513?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"SPARK-513","project":"Unknown","comment_count":2},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Make NexusScheduler more efficient by keeping a list of tasks for each node","comments":"Comment 1 by Patrick McFadin:\nGithub comment from mateiz: Fixed in master.\n\nComment 2 by Patrick McFadin:\nImported from Github issue spark-3, originally reported by mateiz"},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":2}}
{"type":"key_extraction","metadata":{"issue_key":"SPARK-513","project":"Unknown"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Make NexusScheduler more efficient by keeping a list of tasks for each node\n\nRight now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"SPARK-512","project":"Unknown","issue_type":"Bug","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: SPARK-512\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Shuffle operation\n\nDescription:\nThis is a big issue... the shuffle operation needs to be designed first!\n","output":"Issue SPARK-512 is a bug in the Unknown project. Shuffle operation This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"SPARK-512","project":"Unknown","actual_type":"Bug","actual_priority":""},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Shuffle operation\n\nDescription: This is a big issue... the shuffle operation needs to be designed first!","output":{"issue_type":"Bug","priority":"","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"SPARK-512","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: SPARK-512\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Shuffle operation\n\nDescription:\nThis is a big issue... the shuffle operation needs to be designed first!\n","qa_pairs":[{"question":"What is the main problem described in issue SPARK-512?","answer":"Shuffle operation"},{"question":"What is the current status of issue SPARK-512?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue SPARK-512?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"SPARK-512","project":"Unknown","comment_count":2},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Shuffle operation","comments":"Comment 1 by Patrick McFadin:\nGithub comment from mateiz: This is fixed in master now.\n\nComment 2 by Patrick McFadin:\nImported from Github issue spark-4, originally reported by mateiz"},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":2}}
{"type":"summary","metadata":{"issue_key":"SPARK-511","project":"Unknown","issue_type":"Bug","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: SPARK-511\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Unify the API and implementation of ParallelArrays and HdfsFiles\n\nDescription:\nRight now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.\n","output":"Issue SPARK-511 is a bug in the Unknown project. Unify the API and implementation of ParallelArrays and HdfsFiles This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"SPARK-511","project":"Unknown","actual_type":"Bug","actual_priority":""},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Unify the API and implementation of ParallelArrays and HdfsFiles\n\nDescription: Right now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.","output":{"issue_type":"Bug","priority":"","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"SPARK-511","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: SPARK-511\nProject: Unknown\nType: Bug\nPriority: \nStatus: Resolved\nResolution: Fixed\n\nSummary: Unify the API and implementation of ParallelArrays and HdfsFiles\n\nDescription:\nRight now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.\n","qa_pairs":[{"question":"What is the main problem described in issue SPARK-511?","answer":"Unify the API and implementation of ParallelArrays and HdfsFiles"},{"question":"What is the current status of issue SPARK-511?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue SPARK-511?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"SPARK-511","project":"Unknown","comment_count":2},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Unify the API and implementation of ParallelArrays and HdfsFiles","comments":"Comment 1 by Patrick McFadin:\nGithub comment from mateiz: Done in master branch.\n\nComment 2 by Patrick McFadin:\nImported from Github issue spark-5, originally reported by mateiz"},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":2}}
{"type":"key_extraction","metadata":{"issue_key":"SPARK-511","project":"Unknown"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Unify the API and implementation of ParallelArrays and HdfsFiles\n\nRight now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"HADOOP-8","project":"Unknown","issue_type":"Bug","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: HADOOP-8\nProject: Unknown\nType: Bug\nPriority: Major\nStatus: Closed\nResolution: Won't Fix\n\nSummary: NDFS DataNode advertises localhost as it's address\n","output":"Issue HADOOP-8 is a bug in the Unknown project. NDFS DataNode advertises localhost as it's address This issue has been closed as won't fix."}
{"type":"classification","metadata":{"issue_key":"HADOOP-8","project":"Unknown","actual_type":"Bug","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: NDFS DataNode advertises localhost as it's address\n\nDescription: ","output":{"issue_type":"Bug","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"HADOOP-8","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: HADOOP-8\nProject: Unknown\nType: Bug\nPriority: Major\nStatus: Closed\nResolution: Won't Fix\n\nSummary: NDFS DataNode advertises localhost as it's address\n","qa_pairs":[{"question":"What is the main problem described in issue HADOOP-8?","answer":"NDFS DataNode advertises localhost as it's address"},{"question":"What is the current status of issue HADOOP-8?","answer":"The issue is currently Closed and resolved as Won't Fix."},{"question":"Which components are affected by issue HADOOP-8?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"HADOOP-8","project":"Unknown","comment_count":3},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"NDFS DataNode advertises localhost as it's address","comments":"Comment 1 by Peter Sandström:\nfixes the problem by connecting to the NameNode and using the address that the local socket is bound to instead of calling getLocalHost(), beware that this makes machineName in the DataNode constructor unused.\n\nComment 2 by Doug Cutting:\nThe problem with this fix is that, if the namenode is down, the socket creation fails and we fail to launch a datanode.  So it would be great to have a better method than InetAddress.getLocalHost(), that tries harder to return the appropriate IP to advertise to the namenode, but that doesn't require the namenode to be up.\n\nPerhaps this could use NetworkInterfaces.getInetAddresses() and find the first that is not a loopback interface?\n\nComment 3 by Doug Cutting:\nThis is a known JVM bug:\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4665037\n\nSo let's let Sun fix it, rather than fix it ourselves.  One can workaround it by altering one's linux configuration."},"output":{"key_points":["Discussion involves 2 participants","Recent activity suggests ongoing development or discussion"],"participant_count":2,"total_comments":3}}
{"type":"summary","metadata":{"issue_key":"HADOOP-19","project":"Unknown","issue_type":"Bug","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: HADOOP-19\nProject: Unknown\nType: Bug\nPriority: Critical\nStatus: Closed\nResolution: Fixed\n\nSummary: Datanode corruption\n\nDescription:\nOur admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).\n","output":"Issue HADOOP-19 is a bug in the Unknown project. Datanode corruption This issue has been closed as fixed."}
{"type":"classification","metadata":{"issue_key":"HADOOP-19","project":"Unknown","actual_type":"Bug","actual_priority":"Critical"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Datanode corruption\n\nDescription: Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the so","output":{"issue_type":"Bug","priority":"Critical","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"HADOOP-19","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: HADOOP-19\nProject: Unknown\nType: Bug\nPriority: Critical\nStatus: Closed\nResolution: Fixed\n\nSummary: Datanode corruption\n\nDescription:\nOur admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).\n","qa_pairs":[{"question":"What is the main problem described in issue HADOOP-19?","answer":"Datanode corruption"},{"question":"What is the current status of issue HADOOP-19?","answer":"The issue is currently Closed and resolved as Fixed."},{"question":"Which components are affected by issue HADOOP-19?","answer":"No specific components are mentioned."},{"question":"Who is assigned to work on issue HADOOP-19?","answer":"This issue is assigned to Doug Cutting."}]}
{"type":"discussion","metadata":{"issue_key":"HADOOP-19","project":"Unknown","comment_count":1},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Datanode corruption","comments":"Comment 1 by Doug Cutting:\nI added a format command to namenode.  Now one must run 'bin/hadoop namenode -format' whenever a new dfs.data.dir is configured.  A misconfigured namenode will thus no longer create a new empty filesystem and then tell all datanodes to discard their blocks."},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":1}}
{"type":"key_extraction","metadata":{"issue_key":"HADOOP-19","project":"Unknown"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Datanode corruption\n\nOur admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\n\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\n\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\n\n\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"HADOOP-10","project":"Unknown","issue_type":"Bug","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: HADOOP-10\nProject: Unknown\nType: Bug\nPriority: Trivial\nStatus: Closed\nResolution: Fixed\n\nSummary: ndfs.replication is not documented within the nutch-default.xml configuration file.\n\nDescription:\nndfs.replication is not documented within the nutch-default.xml configuration file.\n","output":"Issue HADOOP-10 is a bug in the Unknown project. ndfs.replication is not documented within the nutch-default.xml configuration file. This issue has been closed as fixed."}
{"type":"classification","metadata":{"issue_key":"HADOOP-10","project":"Unknown","actual_type":"Bug","actual_priority":"Trivial"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: ndfs.replication is not documented within the nutch-default.xml configuration file.\n\nDescription: ndfs.replication is not documented within the nutch-default.xml configuration file.","output":{"issue_type":"Bug","priority":"Trivial","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"HADOOP-10","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: HADOOP-10\nProject: Unknown\nType: Bug\nPriority: Trivial\nStatus: Closed\nResolution: Fixed\n\nSummary: ndfs.replication is not documented within the nutch-default.xml configuration file.\n\nDescription:\nndfs.replication is not documented within the nutch-default.xml configuration file.\n","qa_pairs":[{"question":"What is the main problem described in issue HADOOP-10?","answer":"ndfs.replication is not documented within the nutch-default.xml configuration file."},{"question":"What is the current status of issue HADOOP-10?","answer":"The issue is currently Closed and resolved as Fixed."},{"question":"Which components are affected by issue HADOOP-10?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"HADOOP-10","project":"Unknown","comment_count":1},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"ndfs.replication is not documented within the nutch-default.xml configuration file.","comments":"Comment 1 by Doug Cutting:\nThis has been fixed.  The paramter is now dfs.replication in hadoop-default.xml."},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":1}}
{"type":"summary","metadata":{"issue_key":"HADOOP-18","project":"Unknown","issue_type":"Bug","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: HADOOP-18\nProject: Unknown\nType: Bug\nPriority: Critical\nStatus: Closed\nResolution: Invalid\n\nSummary: Crash with multiple temp directories\n\nDescription:\nA brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred.local.dir</name>\n    <value>/local,/local1,/local2</value>\n    <description>The local directory where MapReduce stores intermediate\n    data files.\n    </description>\n  </property>\n\nThis failed with the below exception during either the generate or update phase (not entirely sure which).\n\njava.lang.ArrayIndexOutOfBoundsException\n        at java.util.zip.CRC32.update(CRC32.java:51)\n        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n        at java.io.DataInputStream.readFully(DataInputStream.java:176)\n        at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)\n        at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)\n        at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)\n        at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)\n        at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)\n        at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)\n        at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:604)\n","output":"Issue HADOOP-18 is a bug in the Unknown project. Crash with multiple temp directories This issue has been closed as invalid."}
{"type":"classification","metadata":{"issue_key":"HADOOP-18","project":"Unknown","actual_type":"Bug","actual_priority":"Critical"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Crash with multiple temp directories\n\nDescription: A brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred.local.dir</name>\n    <value>/local,/local1,/local2</value>\n    <description>The local directory where MapReduce stores intermediate\n    data files.\n    </description>\n  </property>\n\nThis failed with the below exception during either the generate or update phase (not entirely sure which).\n\njava.lang.ArrayIndexOutOfBoundsException\n        at java.ut","output":{"issue_type":"Bug","priority":"Critical","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"HADOOP-18","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: HADOOP-18\nProject: Unknown\nType: Bug\nPriority: Critical\nStatus: Closed\nResolution: Invalid\n\nSummary: Crash with multiple temp directories\n\nDescription:\nA brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred.local.dir</name>\n    <value>/local,/local1,/local2</value>\n    <description>The local directory where MapReduce stores intermediate\n    data files.\n    </description>\n  </property>\n\nThis failed with the below exception during either the generate or update phase (not entirely sure which).\n\njava.lang.ArrayIndexOutOfBoundsException\n        at java.util.zip.CRC32.update(CRC32.java:51)\n        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n        at java.io.DataInputStream.readFully(DataInputStream.java:176)\n        at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)\n        at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)\n        at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)\n        at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)\n        at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)\n        at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)\n        at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:604)\n","qa_pairs":[{"question":"What is the main problem described in issue HADOOP-18?","answer":"Crash with multiple temp directories"},{"question":"What is the current status of issue HADOOP-18?","answer":"The issue is currently Closed and resolved as Invalid."},{"question":"Which components are affected by issue HADOOP-18?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"HADOOP-18","project":"Unknown","comment_count":5},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Crash with multiple temp directories","comments":"Comment 1 by Rod Taylor:\nWish there was an \"edit\" option in JIRA.  Obviously it was within the SegmentReader -- though I don't believe it does anything special to try to read the data file off disk.\n\nI do not and have not seen this exception with a single local directory configuration.\n\nComment 2 by Doug Cutting:\nI have sucessfully used mapred.local.dir with multiple values on many occasions.\n\nCan you please try to distill this to an easy to reproduce test-case?  Thanks.\n\nComment 3 by Rod Taylor:\nI enabled multiple local directories on 50% of my machines. This error, which does not appear otherwise (we processed a few hundred million pages without error), happens across all machines BUT always on part-##'s which were created on the split directory boxes.\n\nSecondly, the error only occurs during the segread command, org.apache.nutch.segment.SegmentReader, while retrieving the data out of common storage (NAS in our case).\n\n060203 222553 task_m_4x4zan 0.7223333% /opt/sitesell/sbider_data/nutch/segments/segmentset-2006-02-02254/20060202181143-3/content/part-00001/data:0+21025408\n060203 222553 task_m_4x4zan  Problem reading checksum file: java.io.EOFException. Ignoring.\n060203 222553 task_m_4x4zan  Error running child\n060203 222553 task_m_4x4zan java.lang.ArrayIndexOutOfBoundsException\n060203 222553 task_m_4x4zan     at java.util.zip.CRC32.update(CRC32.java:43)\n060203 222553 task_m_4x4zan     at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n060203 222553 task_m_4x4zan     at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n060203 222553 task_m_4x4zan     at java.io.BufferedInputStream.read1(BufferedInputStream.java:254)\n060203 222553 task_m_4x4zan     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n060203 222553 task_m_4x4zan     at java.io.DataInputStream.readFully(DataInputStream.java:176)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)\n060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)\n060203 222553 task_m_4x4zan     at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)\n060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:603)\n\n060203 222557 task_m_4x4zan done; removing files.\n\nComment 4 by Rod Taylor:\nFinally figured it out. One temp directory was filling up (different sizes) and the fetch was aborting BUT I didn't see this in the logs for the longest time.\n\nThe patch in NUTCH-143 helped track down the issue because it caused the error to be noticed be the scripts driving the code in a location close to where the error took place rather than several steps (possibly many hours) later..\n\nPlease close this bug.\n\nComment 5 by Doug Cutting:\nResoving at Rod's request."},"output":{"key_points":["Discussion involves 2 participants","Recent activity suggests ongoing development or discussion"],"participant_count":2,"total_comments":5}}
{"type":"key_extraction","metadata":{"issue_key":"HADOOP-18","project":"Unknown"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Crash with multiple temp directories\n\nA brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n\n  <property>\n    <name>mapred.local.dir</name>\n    <value>/local,/local1,/local2</value>\n    <description>The local directory where MapReduce stores intermediate\n    data files.\n    </description>\n  </property>\n\nThis failed with the below exception during either the generate or update phase (not entirely sure which).\n\njava.lang.ArrayIndexOutOfBoundsException\n        at java.util.zip.CRC32.update(CRC32.java:51)\n        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n        at java.io.DataInputStream.readFully(DataInpu","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"HADOOP-9","project":"Unknown","issue_type":"Bug","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: HADOOP-9\nProject: Unknown\nType: Bug\nPriority: Minor\nStatus: Closed\nResolution: Fixed\n\nSummary: mapred.local.dir  temp dir. space allocation limited by smallest area\n\nDescription:\nWhen mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is \"round robin starting from a randomish point\".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. \n\nWorkaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.\n\nI wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space. \n\n\n","output":"Issue HADOOP-9 is a bug in the Unknown project. mapred.local.dir  temp dir. space allocation limited by smallest area This issue has been closed as fixed."}
{"type":"classification","metadata":{"issue_key":"HADOOP-9","project":"Unknown","actual_type":"Bug","actual_priority":"Minor"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: mapred.local.dir  temp dir. space allocation limited by smallest area\n\nDescription: When mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is \"round robin starting from a randomish point\".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. \n\nWorkaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.\n\nI wrote a patch to JobConf (currenly being tested) which uses ","output":{"issue_type":"Bug","priority":"Minor","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"HADOOP-9","project":"Unknown"},"instruction":"Answer the questions below using the issue context.","context":"Issue: HADOOP-9\nProject: Unknown\nType: Bug\nPriority: Minor\nStatus: Closed\nResolution: Fixed\n\nSummary: mapred.local.dir  temp dir. space allocation limited by smallest area\n\nDescription:\nWhen mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is \"round robin starting from a randomish point\".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. \n\nWorkaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.\n\nI wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space. \n\n\n","qa_pairs":[{"question":"What is the main problem described in issue HADOOP-9?","answer":"mapred.local.dir  temp dir. space allocation limited by smallest area"},{"question":"What is the current status of issue HADOOP-9?","answer":"The issue is currently Closed and resolved as Fixed."},{"question":"Which components are affected by issue HADOOP-9?","answer":"No specific components are mentioned."},{"question":"Who is assigned to work on issue HADOOP-9?","answer":"This issue is assigned to Ariel Shemaiah Rabkin."}]}
{"type":"discussion","metadata":{"issue_key":"HADOOP-9","project":"Unknown","comment_count":9},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"mapred.local.dir  temp dir. space allocation limited by smallest area","comments":"Comment 1 by Ariel Shemaiah Rabkin:\nThis is a less pressing issue these days, since you can pass a size to the local dir allocator and it'll do the right thing.\nDo people think this is still worth fixing?   It should be straightforward to do something roulette-y in LocalDirAllocator.getLocalPathForWrite for unknown-size writes.\n\nComment 2 by Ariel Shemaiah Rabkin:\nPatch fixing this issue.\n\nComment 3 by Ariel Shemaiah Rabkin:\ndowngrading priority\n\nComment 4 by Hadoop QA:\n+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12386584/hadoop9.patch\n  against trunk revision 678593.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2920/console\n\nThis message is automatically generated.\n\nComment 5 by Owen O'Malley:\nI just committed this. Thanks, Ari!\n\nComment 6 by Hudson:\nIntegrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])\n\nComment 7 by Paul Baclace:\nYou're welcome, Owen!   (I wrote the roulette selection patch.)\n\n\nComment 8 by Ariel Shemaiah Rabkin:\nPaul -- I'm confused. Are you sure this is the issue you meant to comment on?  The patch on this Jira wasn't yours. \n\nComment 9 by Paul Baclace:\nAri,\n\nThis issue was originally NUTCH-181 before Hadoop was split off.  I wrote a patch Dec. 29 2005 and used it at archive.org Jan-Feb 2006.  Looking at my old notes, I created this issue on Jan. 11 2006, and prepared the patch on Feb. 28 2006, but it was either lost in a Jira transition or the attachment somehow failed.  \n\nWhen I looked at your patch yesterday, it was similar enough to what I remembered (5 years ago) that I thought it must be a revision of the patch I did.  Today I found my source and 2005-2006 work notes and it is clear that you implemented the change without seeing mine.\n\nThanks for doing it in same \"roulette-y\" spirit of my lost patch! \n"},"output":{"key_points":["Discussion involves 5 participants","Recent activity suggests ongoing development or discussion"],"participant_count":5,"total_comments":9}}
{"type":"key_extraction","metadata":{"issue_key":"HADOOP-9","project":"Unknown"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"mapred.local.dir  temp dir. space allocation limited by smallest area\n\nWhen mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is \"round robin starting from a randomish point\".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. \n\nWorkaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.\n\nI wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space. \n\n","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-2","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-2\nProject: Kafka\nType: Improvement\nPriority: Minor\nStatus: Resolved\nResolution: Information Provided\n\nSummary: a restful producer API\n\nDescription:\nIf Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language.\n","output":"Issue KAFKA-2 is a improvement in the Kafka project. a restful producer API This issue has been resolved as information provided."}
{"type":"classification","metadata":{"issue_key":"KAFKA-2","project":"KAFKA","actual_type":"Improvement","actual_priority":"Minor"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: a restful producer API\n\nDescription: If Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language.","output":{"issue_type":"Improvement","priority":"Minor","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-2","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-2\nProject: Kafka\nType: Improvement\nPriority: Minor\nStatus: Resolved\nResolution: Information Provided\n\nSummary: a restful producer API\n\nDescription:\nIf Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language.\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-2?","answer":"a restful producer API"},{"question":"What is the current status of issue KAFKA-2?","answer":"The issue is currently Resolved and resolved as Information Provided."},{"question":"Which components are affected by issue KAFKA-2?","answer":"No specific components are mentioned."},{"question":"Who is assigned to work on issue KAFKA-2?","answer":"This issue is assigned to Dru Panchal."}]}
{"type":"discussion","metadata":{"issue_key":"KAFKA-2","project":"KAFKA","comment_count":2},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"a restful producer API","comments":"Comment 1 by Anand Iyer:\nSince the Kafka APIs are undergoing changes in upcoming versions, this JIRA becomes more important. Kafka customers are asking for it. This is important for increased adoption of Kafka.\n\nComment 2 by Dru Panchal:\n[~anandriyer] I would like to resolve this JIRA because the Kafka REST Proxy covers your desired use case.\n\nhttp://docs.confluent.io/2.0.0/kafka-rest/docs/index.html\n"},"output":{"key_points":["Discussion involves 2 participants","Recent activity suggests ongoing development or discussion"],"participant_count":2,"total_comments":2}}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-2","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"a restful producer API\n\nIf Kafka server supports a restful producer API, we can use Kafka in any programming language without implementing the wire protocol in each language.","output":{"labels":[],"components":[],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-3","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-3\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Abandoned\n\nSummary: Consumer needs a pluggable decoder\n\nDescription:\nKafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T).\n","output":"Issue KAFKA-3 is a improvement in the Kafka project. Consumer needs a pluggable decoder This issue has been resolved as abandoned."}
{"type":"classification","metadata":{"issue_key":"KAFKA-3","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Consumer needs a pluggable decoder\n\nDescription: Kafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T).","output":{"issue_type":"Improvement","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-3","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-3\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Abandoned\n\nSummary: Consumer needs a pluggable decoder\n\nDescription:\nKafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T).\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-3?","answer":"Consumer needs a pluggable decoder"},{"question":"What is the current status of issue KAFKA-3?","answer":"The issue is currently Resolved and resolved as Abandoned."},{"question":"Which components are affected by issue KAFKA-3?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"KAFKA-3","project":"KAFKA","comment_count":9},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Consumer needs a pluggable decoder","comments":"Comment 1 by Joel Jacob Koshy:\n\nHere is a first stab at plugging in a decoder for the consumer API.  The\nhtml docs/quickstart also need to be updated after this is finalized, so\nthis is more of a checkpoint review request.\n\nThe consumer API becomes a bit clunky with the parameterized type so if\nanyone has ideas on making this simpler/cleaner please comment.\n\nThere is an example of the API in action in the new test under\nZookeeperConsumerConnectorTest, but here is a summary:\n\n// create consumerConfig and set config.deserializerClass to\n// MyDecoder (extends Decoder<MyRichType>)\n\nval consumer = new ZookeeperConsumerConnector[MyRichType](consumerConfig)\nval streams = consumer.createMessageStreams(topicMap) // each stream is of type KafkaMessageStream[MyRichType]\nfor (stream : streams) {\n    for (myRichTypeMessage <- stream) {\n        // use myRichTypeMessage: MyRichType\n    }\n}\n\n- With this approach, createMessageStreams can handle at most one decoder\n  class. If the client needs different decoders for different topics, the\n  client will need to create separate consumer connectors with a different\n  deserializer for each one. One possible way to avoid that would be to make\n  the deserializer config a topic:deserializer map, and move the type\n  parameter to the createMessageStreams method. The client will still need\n  to call createMessageStreams for each deserializer type, but will not need\n  multiple connectors objects - not sure if this adds much value though and\n  it makes the config more cumbersome.\n\n- For ConsoleConsumer, the MessageFormatter cannot be parameterized from the\n  config. So that poses a bit of a redundancy problem in that the formatter\n  presumably needs to decode the message. I guess you can have a decoder\n  implementation that implements both the decoder and formatter traits, and\n  have the formatter implementation use the decoder if necessary.\n\nSince this is a client API change, it would be great to discuss and resolve\nany concerns about it at this point - should we post this comment on\nkafka-user as well?\n\n\nComment 2 by Jay Kreps:\nOne way to do this would be to make use of type inference off the method arguments. For example, you could make the api be\nval firstStreamConfig = new StreamConfig[String](topic = \"my_topic\", parallelism = 5, decoder=new StringDecoder())\nval secondStreamConfig = new StreamConfig[Integer](topic = \"your_topic\", parallelism = 5, decoder=new IntDecoder())\nval streamSet: StreamSet = consumer.createMessageStreams(firstStreamConfig, secondStreamConfig)\nval streamOne: List[KafkaMessageStream[String]] = streamSet.get(firstStreamConfig)\n\nHere the type of the iterator is actually inferred from the parametrization on the config object passed in. This trick is used a lot in java APIs to help pass through type parameters. The other advantage is that it gives us a general per-topic config object. Currently we don't have a very clean way to do per-topic config.\n\nAnother way to do this would be to ask if there is a way to just make the stream creation be one at a time without causing a \"rebalance storm\" for the clients. In that case you could do \nval consumer = new ZookeeperConsumerConnector[MyRichType](consumerConfig)\nval streamsA = consumer.createMessageStream(\"topic_a\", decoder, parallelism)\nval streamsB = consumer.createMessageStream(\"topic_a\", decoder, parallelism)\nTo my eyes this later approach is much more friendly, especially in the common case where you just have one topic to consume from. I think the reason we didn't do this was to avoid having a sequence of these cause a ton of rebalancing activity. I wonder if there is a direct fix for that.\n\nComment 3 by Joel Jacob Koshy:\nThanks for the tip - that's an interesting idea. I had actually tried using type inference to avoid having to specify the default [Message] when a decoder is not required, but couldn't get it to work - I don't recall exactly why, but now that I read your comment, I think parameterizing the API methods instead of the class makes it more tractable. I'll experiment tomorrow.\n\nComment 4 by Joel Jacob Koshy:\nI tried various experiments for this - the difficulty lies in bringing the Java\nAPI up to par with the Scala API mainly due to limitations in (or my\nunderstanding of) the interoperability of Java-Scala generics and default\narguments.\n\nThe current consumer API is:\n\n    createMessageStreams(topicCountMap: Map[String, Int]): Map[String,\n        List[KafkaMessageStream]]\n\nWith consumer decoders, we want to be able to return a Map[String,\nList[KafkaMessageStream[T]]], and if possible have different types bound to\nT in the map. This is doable in Scala, but I don't see how we can support an\nequally flexible API in Java.\n\nSo I think our alternatives are:\n\n1 - Use type inference, and allow disparity between the Scala and Java APIs\n2 - Go with a simpler API that requires an explicit type parameter, but is\n    consistent across Java/Scala\n3 - Other thoughts?\n\nFor the first approach, the API could be:\n\n    createMessageStreams[T](topicCountMap: Map[String, StreamConfig[T]]):\n        Map[String, List[KafkaMessageStream[T]]]\n\nThere are a couple of issues in getting this to work. Although it works well\non the Scala side, Java is more limited. E.g., if the type parameter is\ndeclared covariant (i.e., +T) with Scala it is possible to mix different\ndecoder types in a single call to createMessageStreams, but it is not\npossible do this cleanly in Java. Anyway, I think this use case would be\nrare.  More importantly, the other useful benefit of this approach (default\ntype if decoder is not specified) is also not available to the Java API,\nsince Java doesn't understand default arguments. I tried a wrapper factory\nobject to make StreamConfigs, but ultimately an explicit type and casting is\nrequired on the Java side.\n\nThe second approach would be along the following lines:\n\nConsumer connector provides:\n\n    createMessageStreams[T](topicCountMap: Map[String, Int], decoder:\n        Decoder[T]): Map[String, List[KafkaMessageStream[T]]]\n\nSo pretty much the existing API with the addition of the parameterized type\nand the decoder argument.\n\nScala usage:\n\n// c is a ConsumerConnector\n\nval intStreamMap = c.createMessageStreams(intTopicMap, new IntDecoder)\nval floatStreamMap = c.createMessageStreams(floatTopicMap, new FloatDeocder)\nval aFloatTopicStreams = floatStreamMap.get(\"aFloatTopic\")\n\nJava usage:\n\njava.util.Map<String, List<KafkaMessageStream<Float>>> floatStreamMap =\n    c.createMessageStreams(floatTopicMap, new FloatDecoder());\n\nKafkaMessageStream<Float> aFloatTopicStreams =\n    floatStreamMap.get(\"aFloatTopic\");\n\nThoughts?\n\n\nComment 5 by Jun Rao:\nI vote for the second approach. The API is simpler and is consistent between java and scala.\n\nComment 6 by Joel Jacob Koshy:\nHere is an updated patch. (Btw, I added a new method to the Java consumer connector interface to allow creating streams without having to specify the default decoder each time.)\n\nComment 7 by Jun Rao:\nOverall, the patch looks good. Some minor comments:\n\n1. Does ZookeeperConsumerConnector.consume need a default decoder? The caller always passes in a decoder.\n\n2. Shouldn't ConsumerShell use StringDecoder?\n\n3. Consumer.java doesn't need to import kafka.serializer.DefaultDecoder.\n\n4. It seems you don't need the default decoder in ZookeeperConsumerConnector.createMessageStreams, since the default decoder is already defined in the trait.\n\nComment 8 by Joel Jacob Koshy:\nMade the changes in this patch. For 2, either way works, but makes more sense to use StringDecoder now that we have the decoder in the consumer api.\n\nComment 9 by Jun Rao:\nThanks, Joel. Just committed this."},"output":{"key_points":["Discussion involves 3 participants","Recent activity suggests ongoing development or discussion"],"participant_count":3,"total_comments":9}}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-3","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Consumer needs a pluggable decoder\n\nKafka producer allows a user to plug in an encoder (from type T to Message). We need to do the same thing on the consumer side, by allowing a user to plug in a decoder (from Message to type T).","output":{"labels":[],"components":[],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-1","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-1\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Invalid\n\nSummary: The log4j appender still uses the SyncProducer API\n\nDescription:\nThe log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead.\n","output":"Issue KAFKA-1 is a improvement in the Kafka project. The log4j appender still uses the SyncProducer API This issue has been resolved as invalid."}
{"type":"classification","metadata":{"issue_key":"KAFKA-1","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: The log4j appender still uses the SyncProducer API\n\nDescription: The log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead.","output":{"issue_type":"Improvement","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-1","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-1\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Invalid\n\nSummary: The log4j appender still uses the SyncProducer API\n\nDescription:\nThe log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead.\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-1?","answer":"The log4j appender still uses the SyncProducer API"},{"question":"What is the current status of issue KAFKA-1?","answer":"The issue is currently Resolved and resolved as Invalid."},{"question":"Which components are affected by issue KAFKA-1?","answer":"No specific components are mentioned."}]}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-1","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"The log4j appender still uses the SyncProducer API\n\nThe log4j appender still uses the SyncProducer API. Change it to use the Producer API using the StringEncoder instead.","output":{"labels":[],"components":[],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-4","project":"KAFKA","issue_type":"Bug","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-4\nProject: Kafka\nType: Bug\nPriority: Minor\nStatus: Resolved\nResolution: Fixed\n\nSummary: Confusing Error mesage from producer when no kafka brokers are available\n\nDescription:\nIf no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 \nValid values are > 0 \nat kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) \nat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32) \nat scala.collection.TraversableLike$class.map(TraversableLike.scala:206) \nat scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32) \nat kafka.producer.Producer.send(Producer.scala:102) \nat kafka.javaapi.producer.Producer.send(Producer.scala:101) \nat com.linkedin.nusviewer.PublishTestMessage.main(PublishTestMessage.java:45) \n\nThis is confusing. The problem is that no brokers are available, we should make this more clear.\n","output":"Issue KAFKA-4 is a bug in the Kafka project. Confusing Error mesage from producer when no kafka brokers are available This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"KAFKA-4","project":"KAFKA","actual_type":"Bug","actual_priority":"Minor"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Confusing Error mesage from producer when no kafka brokers are available\n\nDescription: If no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 \nValid values are > 0 \nat kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \na","output":{"issue_type":"Bug","priority":"Minor","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-4","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-4\nProject: Kafka\nType: Bug\nPriority: Minor\nStatus: Resolved\nResolution: Fixed\n\nSummary: Confusing Error mesage from producer when no kafka brokers are available\n\nDescription:\nIf no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 \nValid values are > 0 \nat kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) \nat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32) \nat scala.collection.TraversableLike$class.map(TraversableLike.scala:206) \nat scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32) \nat kafka.producer.Producer.send(Producer.scala:102) \nat kafka.javaapi.producer.Producer.send(Producer.scala:101) \nat com.linkedin.nusviewer.PublishTestMessage.main(PublishTestMessage.java:45) \n\nThis is confusing. The problem is that no brokers are available, we should make this more clear.\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-4?","answer":"Confusing Error mesage from producer when no kafka brokers are available"},{"question":"What is the current status of issue KAFKA-4?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue KAFKA-4?","answer":"No specific components are mentioned."}]}
{"type":"discussion","metadata":{"issue_key":"KAFKA-4","project":"KAFKA","comment_count":1},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Confusing Error mesage from producer when no kafka brokers are available","comments":"Comment 1 by Sönke Liebau:\nCurrent error message when no broker is available is:\r\n\r\n{code}\r\nWARN Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\r\n{code}\r\n\r\nThis message was introduced by KAFKA-5179, so I think it is save to say that we can close this ticket as well with the same fix version. Before that there were other error messages that also improved upon this message, but I don't think we need to provide the entire history here..\r\n"},"output":{"key_points":["Discussion involves 1 participants","Recent activity suggests ongoing development or discussion"],"participant_count":1,"total_comments":1}}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-4","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Confusing Error mesage from producer when no kafka brokers are available\n\nIf no kafka brokers are available the producer gives the following error: \n\nException in thread \"main\" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 \nValid values are > 0 \nat kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) \nat kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) \nat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) \nat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32) \nat scala.collection.TraversableLike$class.map(TraversableLike.scala:206) \nat scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32) \nat kafka.producer.Producer.send(Producer.scala:102) \nat kafka.javaapi.producer.Producer.send(Producer.scala:101) \na","output":{"labels":[],"components":[],"issue_type":"Bug"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-8","project":"KAFKA","issue_type":"Improvement","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-8\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Closed\nResolution: Fixed\n\nSummary: Upgrade to Zookeeper 3.3.3\n","output":"Issue KAFKA-8 is a improvement in the Kafka project. Upgrade to Zookeeper 3.3.3 This issue has been closed as fixed."}
{"type":"classification","metadata":{"issue_key":"KAFKA-8","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Upgrade to Zookeeper 3.3.3\n\nDescription: ","output":{"issue_type":"Improvement","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-8","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-8\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Closed\nResolution: Fixed\n\nSummary: Upgrade to Zookeeper 3.3.3\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-8?","answer":"Upgrade to Zookeeper 3.3.3"},{"question":"What is the current status of issue KAFKA-8?","answer":"The issue is currently Closed and resolved as Fixed."},{"question":"Which components are affected by issue KAFKA-8?","answer":"No specific components are mentioned."}]}
{"type":"summary","metadata":{"issue_key":"KAFKA-7","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-7\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Fixed\n\nSummary: javaapi MessageSet doesn't need to implement writeTo socket\n\nDescription:\njavaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi.MessageSet is always converted to the scala version before being sent to socket. \ndef writeTo(channel: WritableByteChannel, offset: Long, maxSize: Long): Long\n","output":"Issue KAFKA-7 is a improvement in the Kafka project. javaapi MessageSet doesn't need to implement writeTo socket This issue has been resolved as fixed."}
{"type":"classification","metadata":{"issue_key":"KAFKA-7","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: javaapi MessageSet doesn't need to implement writeTo socket\n\nDescription: javaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi.MessageSet is always converted to the scala version before being sent to socket. \ndef writeTo(channel: WritableByteChannel, offset: Long, maxSize: Long): Long","output":{"issue_type":"Improvement","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-7","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-7\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Fixed\n\nSummary: javaapi MessageSet doesn't need to implement writeTo socket\n\nDescription:\njavaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi.MessageSet is always converted to the scala version before being sent to socket. \ndef writeTo(channel: WritableByteChannel, offset: Long, maxSize: Long): Long\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-7?","answer":"javaapi MessageSet doesn't need to implement writeTo socket"},{"question":"What is the current status of issue KAFKA-7?","answer":"The issue is currently Resolved and resolved as Fixed."},{"question":"Which components are affected by issue KAFKA-7?","answer":"No specific components are mentioned."}]}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-7","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"javaapi MessageSet doesn't need to implement writeTo socket\n\njavaapi MessageSet (and its subclasses) doesn't need to implement the following method, since a javaapi.MessageSet is always converted to the scala version before being sent to socket. \ndef writeTo(channel: WritableByteChannel, offset: Long, maxSize: Long): Long","output":{"labels":[],"components":[],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-6","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-6\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Invalid\n\nSummary: Support replicating from more than 1 remote kafka cluster\n\nDescription:\nIt would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters.\n","output":"Issue KAFKA-6 is a improvement in the Kafka project. Support replicating from more than 1 remote kafka cluster This issue has been resolved as invalid."}
{"type":"classification","metadata":{"issue_key":"KAFKA-6","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Support replicating from more than 1 remote kafka cluster\n\nDescription: It would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters.","output":{"issue_type":"Improvement","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-6","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-6\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Invalid\n\nSummary: Support replicating from more than 1 remote kafka cluster\n\nDescription:\nIt would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters.\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-6?","answer":"Support replicating from more than 1 remote kafka cluster"},{"question":"What is the current status of issue KAFKA-6?","answer":"The issue is currently Resolved and resolved as Invalid."},{"question":"Which components are affected by issue KAFKA-6?","answer":"No specific components are mentioned."}]}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-6","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Support replicating from more than 1 remote kafka cluster\n\nIt would be good if KafkaServerStartable can take multiple consumerConfig and replicate data from multiple remote kafka clusters.","output":{"labels":[],"components":[],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-5","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-5\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Invalid\n\nSummary: Continuous log flusher\n\nDescription:\nToday, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy. A better way is do have a background thread that keeps flushing dirty logs as faster as the underlying storage allows.\n","output":"Issue KAFKA-5 is a improvement in the Kafka project. Continuous log flusher This issue has been resolved as invalid."}
{"type":"classification","metadata":{"issue_key":"KAFKA-5","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Continuous log flusher\n\nDescription: Today, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy. A better way is do have a background thread that keeps flushing dirty logs as faster as the underlying storage allows.","output":{"issue_type":"Improvement","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-5","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-5\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Invalid\n\nSummary: Continuous log flusher\n\nDescription:\nToday, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy. A better way is do have a background thread that keeps flushing dirty logs as faster as the underlying storage allows.\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-5?","answer":"Continuous log flusher"},{"question":"What is the current status of issue KAFKA-5?","answer":"The issue is currently Resolved and resolved as Invalid."},{"question":"Which components are affected by issue KAFKA-5?","answer":"No specific components are mentioned."}]}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-5","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Continuous log flusher\n\nToday, a kafka log is flushed based on either the number of messages or a certain amount of time elapsed. Setting those numbers properly may not be easy. A better way is do have a background thread that keeps flushing dirty logs as faster as the underlying storage allows.","output":{"labels":[],"components":[],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-10","project":"KAFKA","issue_type":"Improvement","status":"Resolved"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-10\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Won't Fix\n\nSummary: Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution\n\nDescription:\nApache Whirr is a a set of libraries for running cloud services \nhttp://incubator.apache.org/whirr/ \n\nIt is desirable that Kafka's integration with EC2 be Whirr based, rather than the code based solution we currently have in contrib/deploy. \n\nThe code in contrib/deploy will be deleted in 0.6 release\n\nComponents: packaging\n","output":"Issue KAFKA-10 is a improvement in the Kafka project affecting packaging. Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution This issue has been resolved as won't fix."}
{"type":"classification","metadata":{"issue_key":"KAFKA-10","project":"KAFKA","actual_type":"Improvement","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution\n\nDescription: Apache Whirr is a a set of libraries for running cloud services \nhttp://incubator.apache.org/whirr/ \n\nIt is desirable that Kafka's integration with EC2 be Whirr based, rather than the code based solution we currently have in contrib/deploy. \n\nThe code in contrib/deploy will be deleted in 0.6 release","output":{"issue_type":"Improvement","priority":"Major","components":["packaging"],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-10","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-10\nProject: Kafka\nType: Improvement\nPriority: Major\nStatus: Resolved\nResolution: Won't Fix\n\nSummary: Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution\n\nDescription:\nApache Whirr is a a set of libraries for running cloud services \nhttp://incubator.apache.org/whirr/ \n\nIt is desirable that Kafka's integration with EC2 be Whirr based, rather than the code based solution we currently have in contrib/deploy. \n\nThe code in contrib/deploy will be deleted in 0.6 release\n\nComponents: packaging\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-10?","answer":"Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution"},{"question":"What is the current status of issue KAFKA-10?","answer":"The issue is currently Resolved and resolved as Won't Fix."},{"question":"Which components are affected by issue KAFKA-10?","answer":"The affected components are: packaging"}]}
{"type":"discussion","metadata":{"issue_key":"KAFKA-10","project":"KAFKA","comment_count":6},"instruction":"Summarize discussion comments and extract any action items or decisions.","input":{"issue_summary":"Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution","comments":"Comment 1 by Pierre-Yves Ritschard:\nPeople will use any of whirr, pallet, chef or other CM method to deploy kafka - I went with pallet. Any reason to keep a specific means in trunk ? Maybe the whirr bits should be in whirr directly like other apache projects ?\n\nComment 2 by Neha Narkhede:\nYou are right. This JIRA is just a placeholder and will be open until someone adds Kafka in Whirr.\n\nComment 3 by Jakob Homan:\nI had started this a while ago, but hadn't finished it.  I'll see if I can find my work, thought it may be outdated.  The code for starting Kafka in Whirr should definitely be hosted within Whirr itself. See WHIRR-212.  Feel free to start if you'd like.\n\nComment 4 by Andrei Savu:\nJakob let me know if I can help to update the code for latest trunk.\n\nComment 5 by Jordan Moore:\nThis should be closed. Whirr is in the Apache Attic. \n\nComment 6 by Matthias J. Sax:\nApache Whirr has been retired."},"output":{"key_points":["Discussion involves 6 participants","Recent activity suggests ongoing development or discussion"],"participant_count":6,"total_comments":6}}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-10","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Kafka deployment on EC2 should be WHIRR based, instead of current contrib/deploy code based solution\n\nApache Whirr is a a set of libraries for running cloud services \nhttp://incubator.apache.org/whirr/ \n\nIt is desirable that Kafka's integration with EC2 be Whirr based, rather than the code based solution we currently have in contrib/deploy. \n\nThe code in contrib/deploy will be deleted in 0.6 release","output":{"labels":[],"components":["packaging"],"issue_type":"Improvement"}}
{"type":"summary","metadata":{"issue_key":"KAFKA-9","project":"KAFKA","issue_type":"Bug","status":"Closed"},"instruction":"Write a brief (2–3 sentence) summary of the issue, mentioning the main problem and any suggested remediation.","input":"Issue: KAFKA-9\nProject: Kafka\nType: Bug\nPriority: Major\nStatus: Closed\nResolution: Fixed\n\nSummary: Consumer logs ERROR during close\n\nDescription:\nWhen closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)\n","output":"Issue KAFKA-9 is a bug in the Kafka project. Consumer logs ERROR during close This issue has been closed as fixed."}
{"type":"classification","metadata":{"issue_key":"KAFKA-9","project":"KAFKA","actual_type":"Bug","actual_priority":"Major"},"instruction":"Assign a category to this issue (Bug, Feature, Improvement, Task) and indicate priority (Critical, Major, Minor, Trivial).","input":"Title: Consumer logs ERROR during close\n\nDescription: When closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)","output":{"issue_type":"Bug","priority":"Major","components":[],"labels":[]}}
{"type":"qa","metadata":{"issue_key":"KAFKA-9","project":"KAFKA"},"instruction":"Answer the questions below using the issue context.","context":"Issue: KAFKA-9\nProject: Kafka\nType: Bug\nPriority: Major\nStatus: Closed\nResolution: Fixed\n\nSummary: Consumer logs ERROR during close\n\nDescription:\nWhen closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)\n","qa_pairs":[{"question":"What is the main problem described in issue KAFKA-9?","answer":"Consumer logs ERROR during close"},{"question":"What is the current status of issue KAFKA-9?","answer":"The issue is currently Closed and resolved as Fixed."},{"question":"Which components are affected by issue KAFKA-9?","answer":"No specific components are mentioned."}]}
{"type":"key_extraction","metadata":{"issue_key":"KAFKA-9","project":"KAFKA"},"instruction":"List the main technical terms, components, and labels found in this issue.","input":"Consumer logs ERROR during close\n\nWhen closing the consumer, we sometimes get the following error: \n\n[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)","output":{"labels":[],"components":[],"issue_type":"Bug"}}